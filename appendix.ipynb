{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90b65de",
   "metadata": {},
   "source": [
    "# Appendix – Detailed Explanations\n",
    "\n",
    "This appendix contains extended descriptions of the dataset choice, preprocessing decisions, feature engineering, model behavior, and evaluation results. These details are not required for the presentation but provide deeper context and are useful for answering questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbdc42",
   "metadata": {},
   "source": [
    "\n",
    "## A1. Rationale for Choosing the Raw Merged Heart Dataset\n",
    "\n",
    "The project initially used a different heart disease dataset, but that dataset was extremely clean:  \n",
    "- almost no missing values,  \n",
    "- uniform distributions,  \n",
    "- minimal noise,  \n",
    "- and preprocessing steps had little to no effect.\n",
    "\n",
    "Because the dataset was so polished, it did not meaningfully demonstrate the impact of preprocessing, feature engineering, or model selection.  \n",
    "To create a more realistic machine learning workflow, I switched to the **raw_merged_heart_dataset**, which includes natural irregularities found in real-world clinical data.\n",
    "\n",
    "This dataset is more suitable because:\n",
    "- Missing values appear in several important clinical variables.\n",
    "- Numerical and categorical distributions are messy and not linearly separable.\n",
    "- Outliers and nonlinear relationships are present.\n",
    "- Categorical variables play a strong predictive role.\n",
    "- Preprocessing choices (e.g., imputation, encoding) genuinely affect performance.\n",
    "\n",
    "Using this dataset made the comparisons between models — especially linear vs. tree-based — much more meaningful and reflective of applied data science challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f929f",
   "metadata": {},
   "source": [
    "\n",
    "## A2. Handling Missing Values: Why LightGBM Uses NaNs\n",
    "\n",
    "Missing values were concentrated in *slope*, *ca*, and *thal* (8–13%). Removing them would have discarded almost 40% of the dataset.\n",
    "\n",
    "**Baseline Models (Logistic Regression, Decision Tree)**\n",
    "\n",
    "These models cannot handle NaN values and require:\n",
    "- numerical imputation (median),  \n",
    "- categorical imputation (most frequent),  \n",
    "- one-hot encoding for categories.\n",
    "\n",
    "This is standard and ensures stable training.\n",
    "\n",
    "**LightGBM**\n",
    "\n",
    "LightGBM handles missing values **natively**. During tree construction:\n",
    "- NaN values are evaluated as a separate branch,\n",
    "- LightGBM automatically assigns missing values to the left or right split based on which yields the best gain.\n",
    "\n",
    "Because missingness itself can carry signal (e.g., missing exercise test results may relate to disease severity), keeping NaNs allows the model to use this information rather than hiding it through imputation.\n",
    "\n",
    "For this reason, **the LightGBM preprocessing pipeline does not impute missing values** and keeps numeric columns unscaled, while categorical variables are ordinal-encoded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b757f",
   "metadata": {},
   "source": [
    "\n",
    "## A3. Feature Engineering: Clinical Motivation and Model Behavior\n",
    "\n",
    "Feature engineering was performed to explore whether domain-informed transformations could reveal additional signal. The engineered features include:\n",
    "\n",
    "**Ratio Features**\n",
    "\n",
    "- `chol_over_age`\n",
    "- `restbps_over_age`\n",
    "\n",
    "These normalize cholesterol and resting blood pressure by age, reflecting age-adjusted cardiovascular stress.\n",
    "\n",
    "**Heart Rate Reserve Features**\n",
    "\n",
    "- `maxhr_reserve = (220 - age) - thalachh`\n",
    "- `percentage_maxhr = thalachh / (220 - age)`\n",
    "\n",
    "These measure exercise capacity and heart performance relative to predicted maximum heart rate.\n",
    "\n",
    "**Log-Transform**\n",
    "\n",
    "- `oldpeak_log = log1p(oldpeak)`\n",
    "\n",
    "This addresses heavy skew in the ST depression measure.\n",
    "\n",
    "**Interaction Terms**\n",
    "\n",
    "- `cp_exang = cp * exang`\n",
    "- `slope_oldpeak = slope * oldpeak`\n",
    "\n",
    "These capture clinically meaningful interactions between symptoms and exercise responses.\n",
    "\n",
    "**Outcome**\n",
    "\n",
    "Feature engineering produced clinically meaningful variables, several of which appear high in LightGBM’s feature importance rankings.  \n",
    "However, **model accuracy decreased slightly**, showing that:\n",
    "- LightGBM already learns nonlinear interactions internally,\n",
    "- Additional engineered features introduced redundancy and slight noise,\n",
    "- The simplest feature representation (raw features only) generalizes best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99942a1",
   "metadata": {},
   "source": [
    "\n",
    "## A4. Why Feature Engineering Reduced Performance for LightGBM\n",
    "\n",
    "Although engineered features ranked highly in importance, the feature-engineered LightGBM model consistently performed **worse** than the raw-feature model, even after hyperparameter tuning.\n",
    "\n",
    "This is expected for boosted tree models because:\n",
    "\n",
    "1. **Redundancy**\n",
    "\n",
    "LightGBM already constructs interaction-like splits. Ratios and interaction features duplicated patterns the model could infer independently.\n",
    "\n",
    "2. **Increased Dimensionality**\n",
    "\n",
    "More features increase the number of possible splits, which can:\n",
    "- add noise,  \n",
    "- cause small overfitting,  \n",
    "- reduce generalization.\n",
    "\n",
    "3. **Lower Signal-to-Noise Ratio**\n",
    "\n",
    "Some engineered features had weaker predictive power than the raw ones they were derived from, slightly diluting the model’s focus.\n",
    "\n",
    "4. **LightGBM’s inherent flexibility**\n",
    "\n",
    "Since LightGBM naturally captures nonlinear structure, manual feature engineering rarely provides large gains unless the engineered features encode domain relationships the model cannot infer automatically.\n",
    "\n",
    "Thus, the **non-FE LightGBM model** was selected as the final model: it is simpler, cleaner, and performs best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55b0ef",
   "metadata": {},
   "source": [
    "## A5. Limitations of the Current Approach\n",
    "\n",
    "Despite strong results, several limitations remain:\n",
    "\n",
    "1. **Dataset Size**\n",
    "\n",
    "With ~2000 rows, performance may vary depending on the train-test split. The model could benefit from more data, especially more positive cases.\n",
    "\n",
    "2. **Single Dataset**\n",
    "\n",
    "The model is evaluated on one dataset from one population.  \n",
    "Generalization to other hospitals, demographics, or data collection protocols is not guaranteed.\n",
    "\n",
    "3. **Missingness Patterns Not Modeled Explicitly**\n",
    "\n",
    "Although LightGBM handles NaNs well, the underlying *reason* for missingness (e.g., clinical workflow) was not analyzed.\n",
    "\n",
    "4. **Limited Model Family**\n",
    "\n",
    "The project focused on a small number of models: logistic regression, decision tree, and LightGBM.  \n",
    "Other competitive tabular models (CatBoost, XGBoost, Random Forests) were not tested.\n",
    "\n",
    "5. **No Probability Calibration**\n",
    "\n",
    "For medical use, calibrated probabilities can matter more than classification accuracy.  \n",
    "This was outside the scope of the project.\n",
    "\n",
    "6. **Interpretability Constraints**\n",
    "\n",
    "While feature importance was examined, deeper interpretability methods such as SHAP values would help clinicians understand risk factors more clearly.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
